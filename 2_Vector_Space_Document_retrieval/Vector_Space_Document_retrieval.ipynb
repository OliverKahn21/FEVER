{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subtask2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "lXuQ3VKZompv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Subtask2"
      ]
    },
    {
      "metadata": {
        "id": "CsNzp2hIom7X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from fever_io import load_dataset_json\n",
        "from math import *\n",
        "import re\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jcsmDAPPlqpg",
        "colab_type": "code",
        "outputId": "ccefd728-6974-470c-be91-a4b7a662553a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VsU-fvN70-M2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PATH = '/content/gdrive/My Drive/IRDM_CHFX2/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dFsH-LiTop2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cosine_similarity(x, y, norm=False):\n",
        "    '''\n",
        "    The input x is the TF-IDF of the claim.\n",
        "    The input y is the TF-IDF of each documents.\n",
        "    The output is the cosine similarity with normalization between 0 and 1.\n",
        "    '''\n",
        "    assert len(x) == len(y), \"len(x) != len(y)\"\n",
        "    zero_list = [0] * len(x)\n",
        "    if x == zero_list or y == zero_list:\n",
        "        return float(1) if x == y else float(0)\n",
        "      \n",
        "    res = np.array([[x[i] * y[i], x[i] * x[i], y[i] * y[i]] for i in range(len(x))])\n",
        "    cos = sum(res[:, 0]) / (np.sqrt(sum(res[:, 1])) * np.sqrt(sum(res[:, 2])))\n",
        "    \n",
        "    return 0.5 * cos + 0.5 if norm else cos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0KOp3BMosMF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extract_wiki(wikipedia_dir): \n",
        "    '''\n",
        "    The input is the path of wiki-pages.\n",
        "    The output is a dictionary which the key is the 'id' and the value is the 'text' which 'text' is not empty.\n",
        "    The output is 3 GB so it is not including in the file.\n",
        "    '''\n",
        "    diction = dict()\n",
        "    for i in range(1,110): ## jsonl file number from 001 to 109\n",
        "        jnum=\"{:03d}\".format(i)\n",
        "        fname=wikipedia_dir+\"wiki-\"+jnum+\".jsonl\"\n",
        "        with open(fname) as f:\n",
        "            line=f.readline()\n",
        "            while line:\n",
        "                data=json.loads(line.rstrip(\"\\n\"))\n",
        "                doc_id=data[\"id\"]\n",
        "                text = data[\"text\"]\n",
        "                if text != \"\":\n",
        "                    diction[doc_id]=text\n",
        "                line=f.readline()\n",
        "    np.save(PATH + \"diction_Subtask2.npy\",diction)\n",
        "    print(\"save complete\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ai3K2GfOotip",
        "colab_type": "code",
        "outputId": "870eec35-10f0-4355-8eed-d55f3cc944f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "extract_wiki(PATH + \"data/wiki-pages/wiki-pages/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tD3vA5MtouvI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dictory(diction):\n",
        "    '''\n",
        "    The purpose of this function is inverted index\n",
        "    The input is the path of 'diction_Subtask2.npy'\n",
        "    The output is a dictionary which the key is the term and the value is the id of the documents including this term. It also print the total number of documents.\n",
        "    The output is 2 GB so it is not including in the file.\n",
        "    '''\n",
        "    data = np.load(diction, allow_pickle=True).item()\n",
        "    dictory = {}\n",
        "    n_ducuments = len(data)\n",
        "    for d in data.items():\n",
        "        text = list(set(d[1].split(' ')))[1:]\n",
        "        for t in text:\n",
        "            t = re.sub(\"[,.。:_=+*&^%$#@!?()<>/`';|]\", \"\", t)\n",
        "            if t.isdigit():\n",
        "                continue\n",
        "            if not t in dictory:\n",
        "                dictory[t] = [d[0]]\n",
        "            else:\n",
        "                dictory[t].append(d[0])\n",
        "    np.save(PATH + \"dictory_Subtask2.npy\",dictory)\n",
        "    print(n_ducuments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sLake9iKowqy",
        "colab_type": "code",
        "outputId": "a51615b5-7191-4b79-d30f-da01d9796ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "create_dictory(PATH + 'diction_Subtask2.npy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5396106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CtLFcxcfoyRj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Subtask2_cossim(claim_id, numberofducuments):\n",
        "    '''\n",
        "    The input is the list of claim 'id' and the total number of documents.\n",
        "    The output is the claim, the top 5 TF-IDF terms in the claim, the TF-IDF of these terms, the five most similar documents with the claim and the cosine similarity between them.\n",
        "    The top 5 TF-IDF terms in the claim and the TF-IDF of these terms is save in the 'Q2_claim_TF-IDF.csv' and the claim with the five most similar documents with the claim is save in the 'Q2_vector_space.csv'.\n",
        "    '''\n",
        "    data = np.load(PATH + 'diction_Subtask2.npy', allow_pickle=True).item()\n",
        "    dictory = np.load(PATH + 'dictory_Subtask2.npy', allow_pickle=True).item()\n",
        "    train_data = load_dataset_json(PATH + 'data/train.jsonl', instance_num=20)\n",
        "\n",
        "    claim = None\n",
        "    for d in train_data:\n",
        "        if d['id'] == claim_id:\n",
        "            d['claim'] = re.sub(\"[,.。:_=+*&^%$#@!?()<>/`';|]\", \"\", d['claim'])\n",
        "            claim = d['claim'].split(' ')\n",
        "            break\n",
        "    print(d['id'])\n",
        "    print(d['claim'])\n",
        "\n",
        "    claim_tfidf = []\n",
        "    keys = []\n",
        "    for c in claim:\n",
        "        tf = claim.count(c) / len(claim)\n",
        "        idf = log((numberofducuments / (1 + (len(dictory[c]) if c in dictory else 0))))\n",
        "        keys.append((c, tf * idf, idf, tf))\n",
        "    keys.sort(key=lambda x: x[1], reverse=True)\n",
        "    keys = keys[:5]\n",
        "    word = [k[0] for k in keys]\n",
        "    vec1 = [k[1] for k in keys] ## vec1 is the list of tf*IDF of top 5 words in the claim.\n",
        "    print(word)\n",
        "    print(vec1)\n",
        "    \n",
        "    document_tfidf = []\n",
        "    for d in data.items():\n",
        "        text = d[1].split(' ')\n",
        "        vec2 = []\n",
        "        for k in keys:\n",
        "            tf = text.count(k[0]) / len(text)\n",
        "            idf = k[2]\n",
        "            vec2.append(tf * idf) ## vec2 is the list of tf*IDF of top 5 words in the document.\n",
        "        sim = cosine_similarity(vec1, vec2)\n",
        "        document_tfidf.append([d[0], sim])\n",
        "    document_tfidf.sort(key=lambda x: x[1], reverse=True)\n",
        "    return document_tfidf[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "de2Ld_54o0Xw",
        "colab_type": "code",
        "outputId": "307210e5-574a-4fb8-a5c6-b3684d2fb4ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "cell_type": "code",
      "source": [
        "index_list = [\n",
        "75397,\n",
        "150448,\n",
        "214861,\n",
        "156709,\n",
        "129629,\n",
        "33078,\n",
        "6744,\n",
        "226034,\n",
        "40190,\n",
        "76253]\n",
        "numberofducuments = 5396106\n",
        "\n",
        "for index in index_list:\n",
        "    print(Subtask2_cossim(index, numberofducuments))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "75397\n",
            "Nikolaj Coster-Waldau worked with the Fox Broadcasting Company\n",
            "['Coster-Waldau', 'Nikolaj', 'Broadcasting', 'Fox', 'Company']\n",
            "[1.4897086502302004, 1.2657387165766936, 0.7486763413296768, 0.7230640262285507, 0.5452877241794746]\n",
            "[['New_Amsterdam_-LRB-TV_series-RRB-', 0.9138333600221531], ['Nikolaj_Coster-Waldau', 0.8763048817667467], ['The_Other_Woman_-LRB-2014_film-RRB-', 0.8763048817667466], ['Game_of_Thrones_-LRB-season_1-RRB-', 0.8570808064872153], ['Simon_Staho', 0.8570808064872153]]\n",
            "150448\n",
            "Roman Atwood is a content creator\n",
            "['Atwood', 'creator', 'content', 'Roman', 'is']\n",
            "[1.5428368732723297, 1.0768405242196075, 0.9376490536887279, 0.7449620407932371, 0.05058654380892037]\n",
            "[['The_O.C._-LRB-season_3-RRB-', 0.8436960639251242], ['Genre_fiction', 0.8433921562727289], ['Joel_Spolsky', 0.8433921562727289], ['Premiere_-LRB-The_O.C.-RRB-', 0.8433921562727289], ['Quetzal_-LRB-disambiguation-RRB-', 0.8433912773406878]]\n",
            "214861\n",
            "History of art includes architecture dance sculpture music painting poetry literature theatre narrative film photography and graphic arts\n",
            "['narrative', 'graphic', 'sculpture', 'photography', 'poetry']\n",
            "[0.374406931501277, 0.3616251425160315, 0.3592837629853828, 0.3569290042709921, 0.32027753391882874]\n",
            "[['History_of_art', 0.948466898156389], ['Leo_Zogmayer', 0.9149764519132724], ['Philip_Pocock', 0.8901847401622802], ['Michael_Salter', 0.7968481012878742], ['BASA_Film', 0.7951666236282958]]\n",
            "156709\n",
            "Adrienne Bailon is an accountant\n",
            "['Bailon', 'Adrienne', 'accountant', 'an', 'is']\n",
            "[2.4070904474995976, 1.8051510847187249, 1.5979326983813698, 0.2404332439354302, 0.06070385257070446]\n",
            "[['Nate_Butler', 0.883829404939039], ['Cheetah-licious_Christmas', 0.8836510042036454], ['Tamera_Mowry', 0.8836510042036453], ['The_Cheetah_Girls_-LRB-group-RRB-', 0.8836507155396623], ['3LW', 0.8836507155396621]]\n",
            "129629\n",
            "Homeland is an American television spy thriller based on the Israeli television series Prisoners of War\n",
            "['Prisoners', 'Homeland', 'spy', 'television', 'television']\n",
            "[0.5651781160595577, 0.5043680290713632, 0.4657749910477197, 0.4484670512395208, 0.4484670512395208]\n",
            "[['Homeland_-LRB-TV_series-RRB-', 0.947462828183397], ['Gideon_Raff', 0.9045172069264943], ['Prisoners_of_War_-LRB-TV_series-RRB-', 0.9045172069264943], ['Homeland_-LRB-season_5-RRB-', 0.8862534208656496], ['Homeland_-LRB-season_4-RRB-', 0.8862534208656495]]\n",
            "33078\n",
            "The Boston Celtics play their home games at TD Garden\n",
            "['Celtics', 'TD', 'Garden', 'Boston', 'games']\n",
            "[0.9306782749193042, 0.7775416698709763, 0.605198836056732, 0.5217040598572392, 0.4110125271084818]\n",
            "[['KDrew', 0.9999999999999999], ['1995_NBA_Playoffs', 0.9389823347969607], ['TD_Garden', 0.9166094655894023], ['Dennis_Johnson', 0.8681897887657456], ['Red_Auerbach', 0.8598363491432969]]\n",
            "6744\n",
            "The Ten Commandments is an epic film\n",
            "['Commandments', 'epic', 'Ten', 'film', 'an']\n",
            "[1.377200273833121, 0.9962355224039046, 0.8763855836902286, 0.4445176842187223, 0.17173803138245014]\n",
            "[['Charlton_Heston', 1.0000000000000002], ['Debra_Paget', 1.0000000000000002], ['Fredric_M._Frank', 1.0], ['Albert_Nozaki', 0.9961962134520859], ['Katherine_Orrison', 0.9788310862976817]]\n",
            "226034\n",
            "Tetris has sold millions of physical copies\n",
            "['Tetris', 'millions', 'copies', 'physical', 'sold']\n",
            "[1.4939661461969238, 1.024918475112283, 0.7813296247133124, 0.7799728514888775, 0.6398022802089729]\n",
            "[['Super_Mario_Land', 0.8137537344723657], ['Wii_Sports', 0.8137537344723657], ['Minecraft', 0.8137537344723655], ['Gerasimov', 0.760528394828722], ['Tetris', 0.7595459990297075]]\n",
            "40190\n",
            "Cyndi Lauper won the Best New Artist award at the 27th Grammy Awards in 1985\n",
            "['1985', 'Lauper', 'Cyndi', '27th', 'Artist']\n",
            "[1.0334125426865142, 0.6529385250365892, 0.6408206787976556, 0.48465413964824966, 0.44513991050039275]\n",
            "[['Krystal_Davis', 0.9026511451749162], ['Changing_of_the_Seasons', 0.9026511451749161], ['Jules_Shear', 0.9026511451749161], ['Marshall_Hain', 0.9026511451749161], ['Phil_Collins_discography', 0.9026511451749161]]\n",
            "76253\n",
            "There is a movie called The Hunger Games\n",
            "['Hunger', 'movie', 'Games', 'There', 'called']\n",
            "[1.0924519214367072, 0.6393013267737645, 0.5852862804143423, 0.4639965934553984, 0.41756314963329383]\n",
            "[['Laurie_Spiegel', 0.952771003869627], ['Atlanta_Marriott_Marquis', 0.9127324182325635], ['Black_Widow_Games', 0.9127324182325635], ['Peter_and_the_Starcatchers', 0.9127324182325635], ['Donald_Sutherland', 0.9127324182325633]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PXrjWYXfOFX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### The code below is the original data pre-processing code. Because of the limited memory, I divided it into two function as above."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MdIbUBYto3_p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def create_dictory_1():\n",
        "#     dictory = {}\n",
        "#     path = PATH + 'data/wiki-pages/wiki-pages/'\n",
        "#     files = os.listdir(path)\n",
        "#     D = 0\n",
        "#     documents = []\n",
        "#     for f in files:\n",
        "#         data = load_dataset_json(os.path.join(path, f))\n",
        "#         documents += data\n",
        "#         for d in data:\n",
        "#             D += 1\n",
        "#             text = list(set(d['text'].split(' ')))[1:]\n",
        "#             for t in text:\n",
        "#                 t = re.sub(\"[,.。:_=+*&^%$#@!?()<>/`';|]\", \"\", t)\n",
        "#                 if t.isdigit():\n",
        "#                     continue\n",
        "#                 if not t in dictory:\n",
        "#                     dictory[t] = [d['id']]\n",
        "#                 else:\n",
        "#                     dictory[t].append(d['id'])\n",
        "#     return dictory, documents, D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-6sELUhYo4b4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def Subtask2(claim_id, dictory, documents, D):\n",
        "# \n",
        "#     train_data = load_dataset_json(PATH + 'data/train.jsonl', instance_num=20)\n",
        "# \n",
        "#     claim = None\n",
        "#     for d in train_data:\n",
        "#         if d['id'] == claim_id:\n",
        "#             d['claim'] = re.sub(\"[,.。:_=+*&^%$#@!?()<>/`';|]\", \"\", d['claim'])\n",
        "#             d['claim'] = d['claim'].lower()\n",
        "#             claim = d['claim'].split(' ')\n",
        "#             break\n",
        "#     #print(d['id'] , d['claim'])\n",
        "# \n",
        "#     claim_tfidf = []\n",
        "#     keys = []\n",
        "#     for c in claim:\n",
        "#         tf = claim.count(c) / len(claim)\n",
        "#         idf = log((D / (1 + (len(dictory[c]) if c in dictory else 0))))\n",
        "#         keys.append((c, tf * idf, idf, tf, (len(dictory[c]) if c in dictory else 0)))\n",
        "#     keys.sort(key=lambda x: x[1], reverse=True)\n",
        "#     keys = keys[:5]\n",
        "#     vec1 = [k[1] for k in keys]\n",
        "#     print(keys)\n",
        "# \n",
        "#     document_tfidf = []\n",
        "#     for d in documents:\n",
        "#         text = d['text'].split(' ')\n",
        "#         vec2 = []\n",
        "#         for k in keys:\n",
        "#             tf = text.count(k[0]) / len(text)\n",
        "#             idf = k[2]\n",
        "#             vec2.append(tf * idf)\n",
        "#         sim = cosine_similarity(vec1, vec2)\n",
        "#         document_tfidf.append([d['id'], sim])\n",
        "#     document_tfidf.sort(key=lambda x: x[1], reverse=True)\n",
        "#     return document_tfidf[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvaqtdQ2o7wO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dictory, documents, D = create_dictory()\n",
        "#\n",
        "# print(Subtask2(75397, dictory, documents, D))\n",
        "# print(Subtask2(150448, dictory, documents, D))\n",
        "# print(Subtask2(214861, dictory, documents, D))\n",
        "# print(Subtask2(156709, dictory, documents, D))\n",
        "# print(Subtask2(129629, dictory, documents, D))\n",
        "# print(Subtask2(33078, dictory, documents, D))\n",
        "# print(Subtask2(6744, dictory, documents, D))\n",
        "# print(Subtask2(226034, dictory, documents, D))\n",
        "# print(Subtask2(40190, dictory, documents, D))\n",
        "# print(Subtask2(76253, dictory, documents, D))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}