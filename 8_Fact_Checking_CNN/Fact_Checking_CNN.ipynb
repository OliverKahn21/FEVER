{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subtask8.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Vlr5ZTPF3_HJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from fever_io import load_dataset_json\n",
        "from math import *\n",
        "import re\n",
        "import gensim.models.keyedvectors as word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p20WSAZF39Ic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51bdc443-aa52-4273-8394-8104e8554b91"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LLwiai3b33ym",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PATH = '/content/gdrive/My Drive/IRDM_CHFX2/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9YRR1yae34Bl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def Subtask8_pre_1():\n",
        "#     '''\n",
        "#     The output is a dictionary which the key is the document 'id' and the value is 'lines' in wiki-pages.\n",
        "#     '''\n",
        "#     train_data = load_dataset_json(PATH + 'train.jsonl')\n",
        "# \n",
        "#     evidence=[]\n",
        "#     for d in train_data.items():\n",
        "#         for i in range(5):\n",
        "#             evidence.append(d[1][i])\n",
        "# \n",
        "#     files = os.listdir(PATH + 'data/wiki-pages/wiki-pages/')\n",
        "#     documents = {}\n",
        "#     for i in files:\n",
        "#         with open(os.path.join(PATH + 'data/wiki-pages/wiki-pages/', i)) as fp:\n",
        "#             lines = fp.readlines()\n",
        "#             for line in lines:\n",
        "#                 line = eval(line)\n",
        "#                 if line['id'] in evidence:\n",
        "#                     text = line['lines']\n",
        "#                     documents[line['id']] = text\n",
        "#     with open(PATH + 'Subtask8_pre_1.txt', 'w', encoding='utf-8') as f:\n",
        "#         f.write(str(documents))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nsrbYkCy3694",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Subtask8_pre_1()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z1PZpbDo5FIB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def Subtask8_pre_2():\n",
        "#     '''\n",
        "#     The output is two document. The 'pos.txt' including the claim and evidence sentence. The 'neg.txt' including the negative sample.\n",
        "#     '''\n",
        "#     train_data = load_dataset_json(PATH + 'train.jsonl')\n",
        "# \n",
        "#     with open(PATH + 'Subtask8_pre_1.txt', encoding='utf-8') as f:\n",
        "#         document = eval(f.read())\n",
        "# \n",
        "#     pos = open(PATH + 'pos.txt', 'w', encoding='utf8')\n",
        "#     neg = open(PATH + 'neg.txt', 'w', encoding='utf8')\n",
        "#     for data in train_data:\n",
        "#         if data['label'] != 'NOT ENOUGH INFO':\n",
        "#             claim = data['claim'][:-1].lower()\n",
        "#             fp = pos if data['label'] == 'SUPPORTS' else neg\n",
        "#             fp.write(claim + '\\n')\n",
        "#             for evidence in data['evidence']:\n",
        "#                 # print(evidence)\n",
        "#                 if evidence[0][2]:\n",
        "#                     tmp = evidence[0][2]\n",
        "#                     if tmp in document:\n",
        "#                         # print(document[tmp])\n",
        "#                         line = document[tmp].split('\\n')[evidence[0][3]].replace(str(evidence[0][3]) + '\\t', '')\n",
        "#                         # print(line)\n",
        "#                         fp.write(line + '\\n')\n",
        "#             # break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oIy1-1ij5GSU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Subtask8_pre_2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8qo6KuWM8EY6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cosine_similarity(x, y, norm=False):\n",
        "    '''\n",
        "    The input x is the TF-IDF of the claim.\n",
        "    The input y is the TF-IDF of each documents.\n",
        "    The output is the cosine similarity with normalization between 0 and 1.\n",
        "    '''\n",
        "    assert len(x) == len(y), \"len(x) != len(y)\"\n",
        "    zero_list = [0] * len(x)\n",
        "    if x == zero_list or y == zero_list:\n",
        "        return float(1) if x == y else float(0)\n",
        "      \n",
        "    res = np.array([[x[i] * y[i], x[i] * x[i], y[i] * y[i]] for i in range(len(x))])\n",
        "    cos = sum(res[:, 0]) / (np.sqrt(sum(res[:, 1])) * np.sqrt(sum(res[:, 2])))\n",
        "    \n",
        "    return 0.5 * cos + 0.5 if norm else cos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oEK8Iy9Q8Lqs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dictory():\n",
        "    '''\n",
        "    This function is the summary of data pre-processing in Subtask2.\n",
        "    '''\n",
        "    dictory = {}\n",
        "    path = PATH + 'data/wiki-pages/wiki-pages/'\n",
        "    files = os.listdir(path)\n",
        "    D = 0\n",
        "    documents = []\n",
        "    for f in files:\n",
        "        data = load_dataset_json(os.path.join(path, f))\n",
        "        documents += data\n",
        "        for d in data:\n",
        "            D += 1\n",
        "            text = list(set(d['text'].split(' ')))[1:]\n",
        "            for t in text:\n",
        "                t = re.sub(\"[,.。:_=+*&^%$#@!?()<>/`';|]\", \"\", t)\n",
        "                if t.isdigit():\n",
        "                    continue\n",
        "                if not t in dictory:\n",
        "                    dictory[t] = [d['id']]\n",
        "                else:\n",
        "                    dictory[t].append(d['id'])\n",
        "    print ('complete')\n",
        "    return dictory, documents, D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vzdcSZKF8fn6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_doc(claim_id, dictory, documents, D):\n",
        "    '''\n",
        "    This function is the the optimized cosine similarly function in Subtask2\n",
        "    The output of the function is dictionary which the key is the document id of the 5 most similar documents of the claim and the value is the document 'line' of each 'id'.\n",
        "    '''\n",
        "    train_data = load_dataset_json(PATH + 'data/train.jsonl', instance_num=20)\n",
        "\n",
        "    claim = None\n",
        "    for d in train_data:\n",
        "        if d['id'] == claim_id:\n",
        "            d['claim'] = re.sub(\"[,.。:_=+*&^%$#@!?()<>/`';|]\", \"\", d['claim'])\n",
        "            d['claim'] = d['claim']\n",
        "            claim = d['claim'].split(' ')\n",
        "            break\n",
        "    # print(d['id'] , d['claim'])\n",
        "\n",
        "    claim_tfidf = []\n",
        "    keys = []\n",
        "    for c in claim:\n",
        "        tf = claim.count(c) / len(claim)\n",
        "        idf = log((D / (1 + (len(dictory[c]) if c in dictory else 0))))\n",
        "        keys.append((c, tf * idf, idf, tf, (len(dictory[c]) if c in dictory else 0)))\n",
        "    keys.sort(key=lambda x: x[1], reverse=True)\n",
        "    keys = keys[:5]\n",
        "    vec1 = [k[1] for k in keys]\n",
        "    # print(keys)\n",
        "\n",
        "    document_tfidf = []\n",
        "    for d in documents:\n",
        "        text = d['text'].split(' ')\n",
        "        vec2 = []\n",
        "        for k in keys:\n",
        "            tf = text.count(k[0]) / len(text)\n",
        "            idf = k[2]\n",
        "            vec2.append(tf * idf)\n",
        "        sim = cosine_similarity(vec1, vec2)\n",
        "        document_tfidf.append([d['id'], sim])\n",
        "    document_tfidf.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "\n",
        "    evidence = []\n",
        "    for i in range(5):\n",
        "        name = document_tfidf[i][0]\n",
        "        evidence.append(name)\n",
        "\n",
        "    files = os.listdir(PATH + 'data/wiki-pages/wiki-pages/')\n",
        "    docu = {}\n",
        "    for i in files:\n",
        "        with open(os.path.join(PATH + 'data/wiki-pages/wiki-pages/', i)) as fp:\n",
        "            lines = fp.readlines()\n",
        "            for line in lines:\n",
        "                line = eval(line)\n",
        "                if line['id'] in evidence:\n",
        "                    text = line['lines']\n",
        "                    docu[line['id']] = text\n",
        "    return docu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_xOCUpm6NOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from cnn_pytorch import predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JNR3F3MN6Ry0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Subtak8_test():\n",
        "    '''\n",
        "    The output is a list which including the top n test data.\n",
        "    '''\n",
        "    test_data = []\n",
        "    cnt = 0\n",
        "    with open(PATH + 'data/test.jsonl') as fp:\n",
        "        for line in fp.readlines():\n",
        "            test_data.append(eval(line))\n",
        "            cnt += 1\n",
        "            if cnt == 10:\n",
        "                break\n",
        "    return test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCNDchzhAI1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1ed8c2a-331b-44a4-c628-7e49037dba7e"
      },
      "cell_type": "code",
      "source": [
        "dictory, documents, D = create_dictory()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K5e-P3_j2nv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Subtask8():\n",
        "    '''\n",
        "    Combine the function using in the previous task.\n",
        "    Using a new convolutional neural networks for Sentence Classification.\n",
        "    '''\n",
        "    with open(PATH + 'Q8_result.txt', 'w') as fp:\n",
        "        test_data = Subtak8_test()\n",
        "        for data in test_data:\n",
        "            res = {}\n",
        "            res['test_id'] = data['id']\n",
        "            res['predicted_label'] = predict(data['claim'])\n",
        "            res['predicted_evidence'] = []\n",
        "            docs = calculate_doc(data['claim'], dictory, documents, D)\n",
        "            claim = data['claim'].split(' ')\n",
        "            tfifd_claim = 0\n",
        "            keys = []\n",
        "            for c in claim:\n",
        "                tf = claim.count(c) / len(claim)\n",
        "                idf = log((D / (1 + (len(dictory[c]) if c in dictory else 0))))\n",
        "                keys.append((c, tf * idf, idf))\n",
        "            keys.sort(key=lambda x: x[1], reverse=True)\n",
        "            keys = keys[:5]\n",
        "            vec1 = [k[1] for k in keys]\n",
        "\n",
        "            for d in docs.items():\n",
        "                lines = d[1].split('\\n')\n",
        "                sims = []\n",
        "                for text in lines:\n",
        "                    vec = []\n",
        "                    for k in keys:\n",
        "                        tf = text.count(k[0]) / len(text)\n",
        "                        idf = k[2]\n",
        "                        vec.append(tf * idf)\n",
        "                    sim = cosine_similarity(vec1, vec)\n",
        "                    sims.append(sim)\n",
        "                # print(sims)\n",
        "                index = sims.index(max(sims))\n",
        "                res['predicted_evidence'].append((d[0], index))\n",
        "            fp.write(str(res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dNkgWcPT9F5u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Subtask8()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}